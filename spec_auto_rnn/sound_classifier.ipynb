{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_length = 499\n",
    "test_length = 1000\n",
    "train_steps = 1000\n",
    "\n",
    "hidden_layer = 5\n",
    "discounted_rate = 0.993\n",
    "\n",
    "sampling_rate = 200\n",
    "striding = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Onset-time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def seizure_time_parser(onset_times,patient_number,data_set_number):\n",
    "    temp = str(onset_times[patient_number-1][data_set_number])\n",
    "    curr_onset_time = []\n",
    "    while temp.find('/')!=-1:\n",
    "        curr_onset_time.append(int(temp[0:temp.find('/')]))\n",
    "        temp=temp[temp.find('/')+1:]\n",
    "    curr_onset_time.append(int(float(temp)))\n",
    "    curr_onset_time=np.array(curr_onset_time)\n",
    "    return curr_onset_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "seizure_file = open('/home/maestoj/medical_analysis/project2/seizure_times.csv','r',newline='')\n",
    "reader = csv.reader(seizure_file,delimiter=',')\n",
    "onset_times_temp=[]\n",
    "\n",
    "for row in reader:\n",
    "    onset_times_temp.append(row)\n",
    "    \n",
    "onset_times = []\n",
    "total_patient = len(onset_times_temp)\n",
    "event_number = np.zeros(total_patient,dtype=np.int32)\n",
    "#number of events of (i+1) patient = event_number[i]\n",
    "\n",
    "for i in range(total_patient):\n",
    "    event_number[i] = len(onset_times_temp[i])-1\n",
    "\n",
    "for p in range(total_patient):\n",
    "    temp = []\n",
    "    for d in range(event_number[p]):\n",
    "        temp.append(seizure_time_parser(onset_times_temp,p+1,d+1))\n",
    "    onset_times.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of datasets of patients\n",
      "[ 7  7  6  3 10  1  7 13 16  8  8  5  3  6 31  8  2  4]\n",
      "1st patient's seizure time for each data_set\n",
      "[781]\n",
      "9th patient's seizure time for each data_set\n",
      "[array([200]), array([312]), array([624]), array([762]), array([752]), array([772]), array([884]), array([502]), array([ 94, 258]), array([836]), array([650]), array([636]), array([576]), array([656]), array([638]), array([706])]\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of datasets of patients\")\n",
    "print(event_number)\n",
    "print(\"1st patient's seizure time for each data_set\")\n",
    "print(onset_times[0][1])\n",
    "print(\"9th patient's seizure time for each data_set\")\n",
    "print(onset_times[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def stft_load(patient_number, dataset_number):\n",
    "    input_folder_path = '/home/maestoj/medical_analysis/stft_data/'\n",
    "    input_data_name = '{}_{}.csv'.format(patient_number,dataset_number)\n",
    "    input_data_path = os.path.join(input_folder_path,input_data_name)\n",
    "    input_file = open(input_data_path,'r',newline='')\n",
    "    reader = csv.reader(input_file, delimiter=',')\n",
    "    print(\"{} is loaded\".format(input_data_name))\n",
    "    temp = []\n",
    "    for row in reader:\n",
    "        temp.append(row)\n",
    "    try:\n",
    "        temp = np.array(temp, dtype=np.float32)\n",
    "    except: \n",
    "        for i in range(len(temp)):\n",
    "            for j in range(len(temp[0])):\n",
    "                temp[i][j]=abs(complex(temp[i][j].replace('i','j')))\n",
    "        temp = np.array(temp, dtype=np.float32)\n",
    "        temp = np.transpose(temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def stft_normal(dataset_number):\n",
    "    input_folder_path = '/home/maestoj/medical_analysis/stft_normal/'\n",
    "    input_data_name = '{}.csv'.format(dataset_number)\n",
    "    input_data_path = os.path.join(input_folder_path,input_data_name)\n",
    "    input_file = open(input_data_path,'r',newline='')\n",
    "    reader = csv.reader(input_file, delimiter=',')\n",
    "    print(\"{} is loaded\".format(input_data_name))\n",
    "    temp = []\n",
    "    for row in reader:\n",
    "        temp.append(row)\n",
    "    try:\n",
    "        temp = np.array(temp, dtype=np.float32)\n",
    "    except: \n",
    "        for i in range(len(temp)):\n",
    "            for j in range(len(temp[0])):\n",
    "                temp[i][j]=abs(complex(temp[i][j].replace('i','j')))\n",
    "        temp = np.array(temp, dtype=np.float32)\n",
    "        temp = np.transpose(temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def time_axis_maker(array, sec_per_cell,init = 0):\n",
    "    final = init + sec_per_cell*(len(array)-1)\n",
    "    return np.linspace(init,final,len(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File auto.meta does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-adc9cdbe6d95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrestorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'auto.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrestorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/maestoj/anaconda3/envs/medical/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m   \"\"\"\n\u001b[1;32m   1518\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1520\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/maestoj/anaconda3/envs/medical/lib/python3.5/site-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    404\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File auto.meta does not exist."
     ]
    }
   ],
   "source": [
    "restorer = tf.train.import_meta_graph('auto.meta')\n",
    "sess = tf.Session()\n",
    "restorer.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "inputs = tf.get_collection(\"input\")\n",
    "X = inputs[0]\n",
    "outputs = tf.get_collection(\"hypo\")\n",
    "H = outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train_set and test_set generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_input_data = []\n",
    "train_output_data = []\n",
    "test_input_data = []\n",
    "test_output_data = []\n",
    "one_hot = np.eye(4)\n",
    "for i in range(total_patient):\n",
    "    train_index = 0.7*nsounds[i]\n",
    "    for j in range(event_number[i]):\n",
    "        try :\n",
    "            result = \n",
    "            patient_number = i\n",
    "            dataset_number = j\n",
    "            \n",
    "            \n",
    "            \n",
    "            temp_input = stft_load(patient_number, dataset_number)\n",
    "            temp_output = np.zeros(train_length)\n",
    "                \n",
    "            for k in range(train_length):\n",
    "                if k ==0:\n",
    "                    temp_output[train_length-1]=1\n",
    "                else:\n",
    "                    temp_output[train_length-1-k] = discounted_rate*temp_output[train_length-k]\n",
    "        \n",
    "            \n",
    "            if j < train_index:\n",
    "                train_input_data.append(sess.run(H, feed_dict={X :}))\n",
    "                train_output_data.append(one_hot[i])\n",
    "                \n",
    "            else :\n",
    "                test_input_data.append(sess.run(H, feed_dict={X :}))\n",
    "                test_output_data.append(one_hot[i])\n",
    "                \n",
    "                \n",
    "            normal_dataset_number = \n",
    "            temp_input2 = stft_normal(normal_dataset_number)\n",
    "            temp_output2 = np.zeors(train_length)    \n",
    "            \n",
    "            if j < train_index:\n",
    "                train_input_data.append(sess.run(H, feed_dict={X :}))\n",
    "                train_output_data.append(one_hot[i])\n",
    "                \n",
    "            else :\n",
    "                test_input_data.append(sess.run(H, feed_dict={X :}))\n",
    "                test_output_data.append(one_hot[i])\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found\")\n",
    "test_input_data = np.array(test_input_data)\n",
    "test_output_data = np.array(test_output_data)\n",
    "train_input_data = np.array(train_input_data)\n",
    "train_output_data = np.array(train_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"Size of train and test set\")\n",
    "print(train_input_data.shape)\n",
    "print(train_output_data.shape)\n",
    "print(test_input_data.shape)\n",
    "print(test_output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 2, train_length])\n",
    "y = tf.placeholder(tf.float32, shape=[None, train_length])\n",
    "test_x = tf.placeholder(tf.float32, shape=[None, 2, test_length])\n",
    "\n",
    "rnn_tei_x = tf.transpose(test_x, [2,0,1])\n",
    "rnn_tei_x = tf.reshape(rnn_tei_x, [-1,2])\n",
    "rnn_tei_x = tf.split(0, test_length, rnn_tei_x)\n",
    "\n",
    "rnn_tri_x = tf.transpose(x, [2,0,1])\n",
    "rnn_tri_x = tf.reshape(rnn_tri_x, [-1,2])\n",
    "rnn_tri_x = tf.split(0, train_length, rnn_tri_x)\n",
    "\n",
    "with tf.variable_scope(\"rnn\") as scope:\n",
    "    lstm_cell = rnn_cell.LSTMCell(rnn_size)\n",
    "\n",
    "    W = tf.Variable(tf.truncated_normal([rnn_size,1], stddev=0.01))\n",
    "    b = tf.Variable(tf.constant(0.01, shape=[1]))\n",
    "    \n",
    "    outputs, states = rnn.rnn(lstm_cell, inputs = rnn_tri_x, dtype=tf.float32)\n",
    "    temp_outputs = tf.pack(outputs)\n",
    "    temp_outputs = tf.transpose(temp_outputs, [1,0,2])\n",
    "    temp_outputs = tf.reshape(temp_outputs,[-1,rnn_size])\n",
    "    temp_outputs = tf.matmul(temp_outputs,W)+b\n",
    "    temp_outputs = tf.reshape(temp_outputs,[-1,train_length,1])\n",
    "    print(temp_outputs)\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "    test_outputs, test_states = rnn.rnn(lstm_cell, inputs = rnn_tei_x, dtype =tf.float32)\n",
    "    temp_test_outputs = tf.pack(test_outputs)\n",
    "    temp_test_outputs = tf.transpose(temp_test_outputs, [1,0,2])\n",
    "    temp_test_outputs = tf.reshape(temp_test_outputs,[-1,rnn_size])\n",
    "    temp_test_outputs = tf.matmul(temp_test_outputs,W)+b\n",
    "    temp_test_outputs = tf.reshape(temp_test_outputs,[-1,test_length,1])    \n",
    "    print(temp_test_outputs)\n",
    "    \n",
    "    temp_y = tf.reshape(y,[-1,train_length,1])\n",
    "    print(temp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(temp_outputs-temp_y))\n",
    "train = tf.train.AdamOptimizer(1e-3).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for step in range(train_steps+1):\n",
    "    opt, c = sess.run([train, cost], feed_dict={x: train_input_data, y: train_output_data})\n",
    "    if step%100==0:\n",
    "        print(\"{}th run cost = {}\".format(step,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
